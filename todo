### ✅ To-Do List du Projet RAG "Boilerplate"

---

#### **Phase 2 : Monitoring et Évaluation**

*Objectif : Mesurer objectivement la performance du RAG et l'améliorer sur la base de données concrètes.*

-   **Monitoring et Analyse de la Latence :**
    -   Mettre en place **Prometheus/Grafana/Loki**.
    -   Metrics des APIs rag-core (metrics rag), et llm-gateway (metrics llm)
    -   Dashboard grafana associés, pour chaque services aussi, se servir des metrics RAGAS également

---

#### **Phase 3 : MLOps & Architecture API-First**

*Objectif : Mettre en place l'outillage pour garantir la qualité, la sécurité, l'automatisation et construire une API robuste et sécurisée.*

-   **Sécurité :**
    -   Ajouter la validation du type de fichier avec `python-magic`.
    -   Gestion des Utilisateurs : Mettre en place un système d'authentification de base (ex: JWT) pour identifier les utilisateurs.
    -   Sécuriser les APIs : Implémenter une authentification par token sur les endpoints (/chat, /ingest).
    -   Isolation des Données : Rendre le collection_name et le namespace dynamiques, basés sur l'ID de l'utilisateur, pour garantir la confidentialité des documents.
    -   Gestion des Quotas :
        -   Permettre aux utilisateurs de stocker leur propre clé API de manière sécurisée (chiffrée en base de données).
        -   Modifier le llm-gateway pour qu'il transmette la clé de l'utilisateur au Proxy LiteLLM pour chaque requête.
        -   Ajouter une logique de limitation d'usage (ex: nombre de documents, nombre de requêtes par mois) pour chaque utilisateur.

-   **Mettre en place les Tests (`pytest`) :**
    -   Écrire les **tests unitaires** pour les fonctions critiques (utils, config, parsing des prompts).
    -   Écrire un **test d'intégration** simple pour l'endpoint `/health` et pour vérifier que les endpoints fonctionnent correctement.

-   **Construire la Pipeline de CI/CD (GitHub Actions) :**
    -   Mettre en place le workflow de **CI** qui exécute `lint` et les `tests` en parallèle pour chaque service.
    -   Préparer un squelette de workflow de **CD** pour le déploiement.

---

### **Phase 4 : Architecture de Déploiement et Cloud :**

*Objectif : Préparer et maîtriser le déploiement de l'application sur une infrastructure cloud scalable.*

-   File d'attente des fichiers uploadés pour ingestion avec Redis (simulation EC2)

-   **Préparation pour Kubernetes :**
    -   Installer et configurer **Kind** ou **Minikube**.
    -   Commencer à écrire les manifestes Kubernetes, en utilisant un Ingress Controller (Nginx) comme équivalent du reverse proxy.

---

#### **Phase 5 : Déploiement Cloud-Native sur Kubernetes**

*Objectif : Maîtriser le déploiement, la gestion d'environnements et l'automatisation de bout en bout.*


-   **Écrire les Manifestes complets :**
    -   Créer les fichiers `Deployment`, `Service`, `ConfigMap`, `Secret` pour chaque microservice.

-   **Gérer les Environnements (`staging` vs. `production`) :**
    -   Mettre en place **Kustomize** pour gérer les variations de configuration.

-   **Mettre à jour le Pipeline de CD (Déploiement Continu) :**
    -   Créer un workflow GitHub Actions pour le **Déploiement Continu**.
    -   Automatiser le déploiement sur **staging**.
    -   Mettre en place une **approbation manuelle** pour le déploiement en **production**.

-   **Porter ça sur GCP**

---

#### **Phase 6 : Expansion des Capacités de l'Agent**

*Objectif : Enrichir l'agent avec de nouvelles compétences et sources d'information.*

-   **Ajouter des Outils Externes :**
    -   Intégrer un outil **Web Search** (ex: `Tavily`).
    -   Outil de génération de document pdf.
    -   Outil de génération de présentation Google Slides.

-   **Diversifier les Sources de Données :**
    -   Ajouter l'ingestion depuis des URLs, Google Drive, Slack...
    -   gestion des images (**OCR** (Mistral, Deepseek, Nanonets...), description d'image etc.)

-   **Retrieval filtré :**
    -   Ingestion : Plusieurs vector store (1 par thème des doucments). Au moment de l'ingestion, appel LLM pour déterminer le thème du nouveau doc, si dispo parmis la liste -> ajout au vector store en question, sinon, création d'un nouveau. (Voir simplifier, sans appel LLM, un endpoint qui permet d'ingérer plusieurs documents dans un nouveau vector store avec thème donné manuellement, oui ça peut tout casser si le user fait n'importe quoi mais bon ce serait super utile pour moi.)
    -   Retrieval : Utiliser la requête du query expander (opti de coûts et de requêtes) pour demander de choisir le thème le plus pertinent pour la query parmis la liste de vector store dispo (si aucune alors connaissances du llm pour répondre) pour choisir dans quel vector store aller chercher l'info (gros gain de qualité de réponse potentiel si plusieurs documents sur sujets variés).

-   Voir pour passer au Cloud pour le stockage, un changement des logins dans le .env devrait suffir (code S3 ready).

-   Site internet comme UI.
